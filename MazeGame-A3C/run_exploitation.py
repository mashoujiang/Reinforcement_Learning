from maze_env import Maze
import numpy as np
import a3c
import tensorflow as tf

env = Maze()

S_INFO = env.n_features
A_DIM = env.n_actions
ACTOR_LR_RATE = 0.0001
CRITIC_LR_RATE = 0.001
NN_MODEL = './results/nn_model_ep_100.ckpt'

with tf.Session() as sess:
        actor = a3c.ActorNetwork(sess,
                                state_dim=S_INFO, action_dim=A_DIM,
                                learning_rate=ACTOR_LR_RATE)
        critic = a3c.CriticNetwork(sess,
                                state_dim=S_INFO,
                                learning_rate=CRITIC_LR_RATE)

	# restore neural network
	nn_model = NN_MODEL
	assert nn_model!=None
	saver = tf.train.Saver()
	saver.restore(sess, nn_model)
	print("Model restored.")
	
	s = env.reset()
	env.deiconify()
	total_reward = 0.0
	steps = 0
	while True:
		env.render()
		state = s
		action_prob = actor.predict(np.reshape(state, (1, S_INFO)))
		print action_prob
		current_a = tf.argmax(action_prob)
		s_, reward, done = env.step(current_a)
		s = s_
		total_reward += reward
		steps +=1
		if done:
			break
	print("Game over! Spend {0} steps, total reward = {1}".format(steps, total_reward))
	
	env.destroy()


	
