from env_2048 import GameGrid
import numpy as np
import a3c
import tensorflow as tf
import time

env = GameGrid()

MAX_NUM = env.max_num
S_INFO = int(env.n_features * (np.log2(MAX_NUM) + 1))
A_DIM = env.n_actions
ACTOR_LR_RATE = 0.0001
CRITIC_LR_RATE = 0.001
NN_MODEL = './models/nn_model_ep_10.ckpt'


def one_hot(sess, state):
    state = np.log2(state)
    whereinf = np.isinf(state)
    state[whereinf] = 0
    state = tf.one_hot(state, np.log2(MAX_NUM) + 1)
    state = tf.reshape(state, [-1])
    state = sess.run(state)
    return state


with tf.Session() as sess:
    actor = a3c.ActorNetwork(sess,
                             state_dim=S_INFO, action_dim=A_DIM,
                             learning_rate=ACTOR_LR_RATE)
    critic = a3c.CriticNetwork(sess,
                               state_dim=S_INFO,
                               learning_rate=CRITIC_LR_RATE)

    # restore neural network
    nn_model = NN_MODEL
    assert nn_model is not None
    saver = tf.train.Saver()
    saver.restore(sess, nn_model)
    print("Model restored.")

    s = env.reset()
    total_reward = 0.0
    steps = 0
    while True:
        state = one_hot(sess, s)
        action_prob = actor.predict(np.reshape(state, (1, S_INFO)))
        current_a = np.argmax(action_prob)
        s_, reward, done = env.step(current_a)
        s = s_
        total_reward += reward
        steps += 1
        time.sleep(0.5)
        if done:
            break
    print("Game over! Spend {0} steps, total reward = {1}".format(steps, total_reward))

    env.destroy()
